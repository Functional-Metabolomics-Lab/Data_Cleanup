{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55766981",
   "metadata": {
    "id": "55766981"
   },
   "source": [
    "# Data Clean up\n",
    "Updated on: 2022-08-10 10:20:48 CEST\n",
    "\n",
    "Author: Abzer Kelminal (abzer.shah@uni-tuebingen.de) <br>\n",
    "Input file format: .csv files or .txt files <br>\n",
    "Outputs: .csv files  <br>\n",
    "Dependencies: ggplot2, dplyr, IRdisplay\n",
    "\n",
    "<p style='text-align: justify;'>This Notebook is used for cleaning the feature table, an output of metabolomics experiment, containing all the MS/MS features (or peaks) with their corresponding intensities. The data cleanup steps involved are: 1) Blank removal 2) Imputation 3) Normalisation.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5aebdb",
   "metadata": {
    "id": "df5aebdb"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "Kindly download this Jupyter Notebook and run it in your local computer for successful running of the code. To know more about how to get the Jupyter Notebook running with R code, please have a look at these documents: [GitHub Link for installing Jupyter using Anaconda](https://github.com/Functional-Metabolomics-Lab/Jupyter-Notebook-Installation/blob/main/Anaconda%20with%20R%20kernel%20installation.pdf), [For directly installing Jupyter without Anaconda](https://simply-python.com/2019/06/24/running-r-on-jupyter-notebook-with-r-kernel-no-anaconda/) \n",
    "    \n",
    "This Notebook can also be opened and followed in <b>Google Colab.</b> Further details on using Google Collab and giving your input files are provided in later sections. \n",
    "    \n",
    "</br>    \n",
    "<b><u><font size=3>SPECIAL NOTE:</u> </b>\n",
    "\n",
    "- <b><font size=3> Please read the comments before proceeding with the code and let us know if you run into any errors and if you think it could be commented better. We would highly appreciate your suggestions and comments!! </font> </b>\n",
    "- <b><font size=3> Also please look for any updates in our GitHub page: [Functional-Metabolomics-Lab](https://github.com/Functional-Metabolomics-Lab/Data_Cleanup) </font></b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f59af0-e3d3-41cf-a6d0-e5656c501d8a",
   "metadata": {
    "id": "f7f59af0-e3d3-41cf-a6d0-e5656c501d8a",
    "tags": []
   },
   "source": [
    "# Brief introduction about the Test data:\n",
    "<div class=\"alert alert-block alert-success\"> \n",
    "    \n",
    "- <p style='text-align: justify;'> Bacteria (B.subtilis and E.coli) were treated with a pool of antibiotics (Sulfamethoxazole, sulfadimethoxine, cyproconazole) and \n",
    "    also a xenobiotic, Asulam, taken at a concentration lower than their MIC (minimum inhibitory concentration). </p>\n",
    "- <p style='text-align: justify;'> The samples were collected at different timepoints, the compounds were extracted (with EtOAc) and measured using LC-MS/MS. </p>\n",
    "- <p style='text-align: justify;'> The goal of the experiment was to look for any potential biotransformation that happened with increase in time. eg: Drug or xenobiotic metabolism </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4705b9a",
   "metadata": {
    "id": "a4705b9a"
   },
   "source": [
    "# Package installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8048c07",
   "metadata": {
    "id": "f8048c07"
   },
   "outputs": [],
   "source": [
    "#installing and calling the necessary packages:\n",
    "if (!require(\"ggplot2\")) install.packages(\"ggplot2\")\n",
    "if (!require(\"dplyr\")) install.packages(\"dplyr\")\n",
    "if (!require(\"IRdisplay\")) install.packages(\"IRdisplay\")\n",
    "\n",
    "library(ggplot2) # For plots\n",
    "library(dplyr) # For data manipulation\n",
    "library(IRdisplay) # For displaying outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e7b575",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Global' settings for plot size in the output cell\n",
    "#options(repr.plot.width=10, repr.plot.height=8,res=600) #For google collab\n",
    "options(repr.plot.width=5, repr.plot.height=3) #For Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a01fdb2-e80e-41b6-b6c3-d43c701f4805",
   "metadata": {},
   "source": [
    "# Input files needed for the Notebook:\n",
    "1) <b>Feature table:</b> An output of metabolomics experiment, containing all the features or peaks (LC-MS/MS peaks here) with their corresponding intensities. The feature table used in the test data is obtained by MZmine. </br>\n",
    "    -   Gap-filled (A typical output of MZmine) </br>\n",
    "    -   Non gapfilled (You can still run the script without non-gapfilled feature file) </br>\n",
    "2) <b>Metadata:</b> Created by the user about the files used obtaining the feature table. The columns in a metadata should be created with the following format: filename (1st column having all the filenames in the same order as the columns in feature table), all the other columns with column name such as: ATTRIBUTE_yourDesiredAttribute. </br>\n",
    "\n",
    "Please have a look at the metadata used here for reference. Creating a metadata in the above-mentioned format is necessary for uploading the files in GNPS and to obtain a molecular network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4557e9d6",
   "metadata": {
    "id": "4557e9d6"
   },
   "source": [
    "# Getting the input files:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a31eafc",
   "metadata": {
    "id": "3a31eafc"
   },
   "source": [
    "## 1) Reading the input data using URL (from GitHub):\n",
    "Here, we can directly pull the data files from our Functional Metabolomics Github page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7988ffd6",
   "metadata": {
    "id": "7988ffd6"
   },
   "outputs": [],
   "source": [
    "## Non-gap filled\n",
    "nft_url <- 'https://raw.githubusercontent.com/abzer005/Data_Cleanup/main/MZmine%20Files/20220726_Xenobiotic_metabolism_Nongapfilled_quant.csv'\n",
    "## Gap filled\n",
    "ft_url <- 'https://raw.githubusercontent.com/abzer005/Data_Cleanup/main/MZmine%20Files/20220726_Xenobiotic_metabolism_Gapfilled_quant.csv'\n",
    "md_url <- 'https://raw.githubusercontent.com/abzer005/Data_Cleanup/main/MZmine%20Files/20220726_Xenobiotic_Metabolism_metadata.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad9e910",
   "metadata": {
    "id": "3ad9e910"
   },
   "outputs": [],
   "source": [
    "nft <- read.csv(nft_url, header = T, check.names = F)\n",
    "ft <- read.csv(ft_url, header = T, check.names = F)\n",
    "md <- read.csv(md_url, header = T, check.names = F, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863a44d4",
   "metadata": {
    "id": "863a44d4"
   },
   "source": [
    "## 2) Setting a local working directory and creating an automatic result directory:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eabebd",
   "metadata": {},
   "source": [
    "### 2.1 For direct Jupyter Notebook Users:\n",
    "<p style='text-align: justify;'> <font color='green'> <b> Setting a local working directory with Jupyter Notebook is very easy.</b> You can simply copy the path of the folder containing all your input files in your local computer to the output line of the next cell. It will be set as your working directory (or working folder)  </font></p> \n",
    "For ex: D:\\User\\Project\\Test_Data <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4734f4b6",
   "metadata": {
    "id": "4734f4b6"
   },
   "outputs": [],
   "source": [
    "# setting the current directory as the working directory\n",
    "Directory <- normalizePath(readline(\"Enter the path of the folder with input files: \"),\"/\",mustWork=FALSE)\n",
    "setwd(Directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0436b28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 271,
     "status": "ok",
     "timestamp": 1659599664396,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "e0436b28",
    "outputId": "601a2203-4ae1-4b1c-ffbb-55157ab8ccc4"
   },
   "outputs": [],
   "source": [
    "getwd() #to get the working directory "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AkCZfbLJ4tjm",
   "metadata": {
    "id": "AkCZfbLJ4tjm"
   },
   "source": [
    "### 2.2 For Google Colab Users:\n",
    "<p style='text-align: justify;'> <font color='red'>For Google Colab, it is not possible to access the files from your local computer as it is hosted on Google's cloud server. An easier workaround is to upload the necessary files into the Google colab session using the 'Files' icon on the left as shown in the image. The code in the next cell creates a new folder 'My_TestData' in the Colab space and sets the folder as working directory. Following the steps in the image, you can check in your Colab to see if the folder has been created. Once you see it, simply upload the files from your local PC to the folder 'My_TestData' and then continue running the rest of the script.</font> </p>\n",
    "\n",
    "<p style='text-align: justify;'>SPECIAL NOTE: All the files uploaded to Google Colab would generally disappear after 12 hours. Similarly, all the outputs would be saved only in the Colab, so we need to download them into our local system at the end of our session.</p> \n",
    "\n",
    "[Go to section: Getting outputs from Colab](#colab_output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ce_JdH6r4g9W",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 237,
     "status": "ok",
     "timestamp": 1659600772520,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "Ce_JdH6r4g9W",
    "outputId": "7f643e3e-b545-4c48-910a-086c270d9506"
   },
   "outputs": [],
   "source": [
    "#Only for Google Colab:\n",
    "dir.create(\"/content/My_TestData\", showWarnings = TRUE, recursive = FALSE, mode = \"0777\")\n",
    "setwd(\"/content/My_TestData\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12c5fc4",
   "metadata": {},
   "source": [
    "![Google-Colab Files Upload](https://github.com/abzer005/Images-for-Jupyter-Notebooks/blob/main/StepsAll.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd010938",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 226,
     "status": "ok",
     "timestamp": 1659601931472,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "dd010938",
    "outputId": "b97d0307-3923-401d-f29b-b561de257b88"
   },
   "outputs": [],
   "source": [
    "# Getting all the files in the folder\n",
    "dirs <- dir(path=paste(getwd(), sep=\"\"), full.names=TRUE, recursive=TRUE)\n",
    "folders <- unique(dirname(dirs))\n",
    "files <- list.files(folders, full.names=TRUE)\n",
    "files_1 <- basename((files))\n",
    "files_2 <- dirname((files))\n",
    "# Creating a Result folder\n",
    "dir.create(path=paste(files_2[[1]], \"_Results\", sep=\"\"), showWarnings = TRUE)\n",
    "fName <-paste(files_2[[1]], \"_Results\", sep=\"\")\n",
    "\n",
    "IRdisplay::display(data.frame(INDEX=c(1:length(files_1)),Filename=files_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ab1133",
   "metadata": {
    "id": "55ab1133"
   },
   "source": [
    "In the following line, enter the required file index numbers separated by commas. For example, in our case, as: 1,3. The accepted file formats are csv,txt and tsv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d07e82",
   "metadata": {
    "id": "45d07e82"
   },
   "outputs": [],
   "source": [
    "input <- as.double(unlist(strsplit(readline(\"Specify the file index of gapfilled feature-file, metadata separated by commas:\"), split=\",\")))\n",
    "\n",
    "#Gets the extension of each file. Ex:csv\n",
    "pattern <- c()\n",
    "  for (i in files_1){\n",
    "    sep_file <- substr(i, nchar(i)-2,nchar(i))\n",
    "    pattern <- rbind(pattern,sep_file)\n",
    "  }\n",
    "#pattern\n",
    "\n",
    "ft <- read.csv(files_1[input[1]],sep = ifelse(pattern[input[1]]!=\"csv\",\"\\t\",\",\"), header=TRUE,check.names = FALSE) # By adding 'row.names = 1' in the code, the 1st column 'ID' becomes the row names\n",
    "md <-read.csv(files_1[input[2]], sep = ifelse(pattern[input[2]]!=\"csv\",\"\\t\",\",\"), header=TRUE,check.names = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2d48db",
   "metadata": {
    "id": "2b2d48db"
   },
   "source": [
    "<p style='text-align: justify;'> Since non gapfilled feature file is not a direct output of mzMine, not everybody will have it. Therefore, we have the next cell separately. For the imputation step, we can also impute the feature table without a non-gapfilled feature file.</p>\n",
    "\n",
    " More about this in the later section: [Imputation with a Cutoff LOD](#Impute_LOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00da9db7",
   "metadata": {
    "id": "00da9db7"
   },
   "outputs": [],
   "source": [
    "#If you have non gapfilled feature file:\n",
    "read_cmd <- readline(\"Do you have non gap-filled feature table? Y/N:\")\n",
    "if(read_cmd==\"Y\"){\n",
    "    x <- as.double(readline(\"Enter the ID number of non-gap-filled feature file:\"))\n",
    "    nft<- read.csv(files_1[x],sep=ifelse(pattern[x]!=\"csv\",\"\\t\",\",\"), header = TRUE,check.names = FALSE)\n",
    "}else if(read_cmd != \"N\"){print(\"Please enter either Y or N\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12e24c",
   "metadata": {
    "id": "4f12e24c"
   },
   "source": [
    "Lets check if the data has been read correclty!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429ff705",
   "metadata": {
    "id": "429ff705"
   },
   "outputs": [],
   "source": [
    "head(ft)\n",
    "dim(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864b7231",
   "metadata": {
    "id": "864b7231"
   },
   "outputs": [],
   "source": [
    "head(nft)\n",
    "dim(nft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bf0865",
   "metadata": {
    "id": "a7bf0865"
   },
   "outputs": [],
   "source": [
    "head(md)\n",
    "dim(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19073e0",
   "metadata": {
    "id": "f19073e0"
   },
   "source": [
    "<p style='text-align: justify;'> In the next cell, we are trying to bring the feature table and metadata in the correct format such as the rownames of metadata and column names of feature table are the same. They both are the file names and they need to be the same, as from now on, we will call the columns in our feature table based on our metadata information. Thus, using the metadata, the user can filter their data easily. You can also directly deal with your feature table without metadata by getting your hands dirty with some coding!! But having a metadata improves the user-experience greatly. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ffd93c",
   "metadata": {
    "id": "25ffd93c"
   },
   "outputs": [],
   "source": [
    "#Removing Peak area extensions\n",
    "colnames(ft) <- gsub(' Peak area','',colnames(ft))\n",
    "md$filename<- gsub(' Peak area','',md$filename)\n",
    "\n",
    "#Removing if any NA columns present in the md file\n",
    "md <- md[,colSums(is.na(md))<nrow(md)]\n",
    "\n",
    "#Changing the row names of the files\n",
    "rownames(md) <- md$filename\n",
    "md <- md[,-1]\n",
    "rownames(ft) <- paste(ft$'row ID',round(ft$'row m/z',digits = 3),round(ft$'row retention time',digits = 3), sep = '_')\n",
    "\n",
    "#Picking only the files with column names containing 'mzML'\n",
    "ft <- ft[,grep('mzML',colnames(ft))]\n",
    "\n",
    "# if nft file exists, we perform all the above for nft as well\n",
    "if(exists(\"nft\")==T){\n",
    "    colnames(nft) <- gsub(' Peak area','',colnames(nft))\n",
    "    rownames(nft) <- paste(nft$'row ID',round(nft$'row m/z',digits = 3),round(nft$'row retention time',digits = 3), sep = '_')\n",
    "    nft <- nft[,grep('mzML',colnames(nft))]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82159eb",
   "metadata": {
    "id": "c82159eb"
   },
   "outputs": [],
   "source": [
    "new_ft<- ft[,order(colnames(ft))] #ordering the ft by its column names\n",
    "new_md <-md[order(rownames(md)),] #ordering the md by its row names\n",
    "\n",
    "#lists the colnames(ft) which are not present in md\n",
    "unmatched_ft <- colnames(new_ft)[which(is.na(match(colnames(new_ft),rownames(new_md))))] \n",
    "cat(\"These\", length(unmatched_ft),\"columns of feature table are not present in metadata:\")\n",
    "if((length(unmatched_ft) %% 2) ==0)\n",
    "{matrix(data=unmatched_ft,nrow=length(unmatched_ft)/2,ncol=2)}else\n",
    "{matrix(data=unmatched_ft,nrow=(length(unmatched_ft)+1)/2,ncol=2)}\n",
    "\n",
    "flush.console()\n",
    "Sys.sleep(0.2)\n",
    "\n",
    "#lists the rownames of md which are not present in ft\n",
    "unmatched_md <- rownames(new_md)[which(is.na(match(rownames(new_md),colnames(new_ft))))] \n",
    "cat(\"These\", length(unmatched_md),\"rows of metadata are not present in feature table:\")\n",
    "if((length(unmatched_md) %% 2) ==0){\n",
    "    matrix(data=unmatched_md,nrow=length(unmatched_md)/2,ncol=2)\n",
    "}else{\n",
    "    matrix(data=unmatched_md,nrow=(length(unmatched_md)+1)/2,ncol=2)}\n",
    "\n",
    "#Removing those unmatching columns and rows:\n",
    "if(length(unmatched_ft)!=0){new_ft <- subset(ft, select = -c(which(is.na(match(colnames(ft),rownames(md))))) )}\n",
    "if(length(unmatched_md)!=0){new_md <- md[-c(which(is.na(match(rownames(md),colnames(ft))))),]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881877f6",
   "metadata": {
    "id": "881877f6"
   },
   "outputs": [],
   "source": [
    "#checking the dimensions of our new ft and md:\n",
    "cat(\"The number of rows and columns in our original ft is:\",dim(ft),\"\\n\")\n",
    "cat(\"The number of rows and columns in our new ft is:\",dim(new_ft),\"\\n\")\n",
    "cat(\"The number of rows and columns in our new md is:\",dim(new_md))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed594448",
   "metadata": {
    "id": "ed594448"
   },
   "source": [
    "Notice that the number of columns of feature table is same as the number of rows in our metadata. Now, we have both our feature table and metadata in the same order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01979e55",
   "metadata": {
    "id": "01979e55"
   },
   "outputs": [],
   "source": [
    "new_ft<- new_ft[,order(colnames(new_ft))] #ordering the ft by its column names\n",
    "new_md <-new_md[order(rownames(new_md)),] #ordering the md by its row names\n",
    "#checking if they are the same\n",
    "if(identical(colnames(new_ft),rownames(new_md))==T)\n",
    "   {print(\"The column names of ft and rownames of md are the same\")}else{print(\"The column names of ft and rownames of md are NOT THE SAME!!\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e927cf45",
   "metadata": {
    "id": "e927cf45"
   },
   "source": [
    "Lets check the files once again!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76fb0e3",
   "metadata": {
    "id": "e76fb0e3"
   },
   "outputs": [],
   "source": [
    "head(nft)\n",
    "dim(nft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499d8a48",
   "metadata": {
    "id": "499d8a48"
   },
   "outputs": [],
   "source": [
    "head(new_ft)\n",
    "dim(new_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6027aea7",
   "metadata": {
    "id": "6027aea7"
   },
   "outputs": [],
   "source": [
    "head(new_md)\n",
    "dim(new_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7453c54f",
   "metadata": {},
   "source": [
    "# Creating Functions:\n",
    "<p style='text-align: justify;'> Before getting into the Data cleanup steps, we have created some functions that can be used later for subsetting and plotting. By creating functions, we don't have to write all these big codes multiple times. Instead, we just use the function name. <font color=\"red\">The following cells in this section will not produce any outputs here. </font> The outputs will be produced when we give input variables to these functions in the later sections. </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c54446",
   "metadata": {},
   "source": [
    "1. <b>Function InsideLevels:</b></br>\n",
    "Using this function, we get an idea of the multiple levels in each of the metioned attributes in the metadata as well as the datatype of each attribute.  <font color =\"blue\"> This function takes metadata table as its input. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351e77ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function: InsideLevels\n",
    " InsideLevels <- function(metatable){\n",
    "    lev <- c()\n",
    "    typ<-c()\n",
    "    for(i in 1:ncol(metatable)){\n",
    "      x <- levels(droplevels(as.factor(metatable[,i])))\n",
    "      if(is.double(metatable[,i])==T){x=round(as.double(x),2)}\n",
    "      x <-toString(x)\n",
    "      lev <- rbind(lev,x)\n",
    "      \n",
    "      y <- class(metatable[,i])\n",
    "      typ <- rbind(typ,y)\n",
    "    }\n",
    "    out <- data.frame(INDEX=c(1:ncol(metatable)),ATTRIBUTES=colnames(metatable),LEVELS=lev,TYPE=typ,row.names=NULL)\n",
    "    return(out)\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df664d3",
   "metadata": {},
   "source": [
    "2. <b>Function SubsetLevels:</b></br>\n",
    "Using this function, we can subset a dataframe using multiple attributes at a time as defined by the user. In addition to that, we can mention whether we want to keep or exclude certain levels within each selected attribute. <font color =\"blue\"> It takes metadata table as its input. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fe8e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function: SubsetLevels  \n",
    "SubsetLevels <- function(metatable){\n",
    "    IRdisplay::display(InsideLevels(metatable)) # use show() when working in RStudio\n",
    "    Condition <- as.double(unlist(strsplit(readline(\"Enter the IDs of interested attributes to subset (separaed by commas if more than one attribute):\"), split=',')))\n",
    "    \n",
    "    for( i in 1:length(Condition)){\n",
    "      list_final <- c() \n",
    "      #Shows the different levels within each selected condition:\n",
    "      Levels_Cdtn <- levels(droplevels(as.factor(metatable[,Condition[i]])))\n",
    "      subset_meta <- data.frame(Index=1:length(Levels_Cdtn),Levels_Cdtn)\n",
    "      colnames(subset_meta)[2] <- paste(\"Levels_\",colnames(metatable[Condition[i]]))\n",
    "      IRdisplay::display(subset_meta) # use show() when working in RStudio\n",
    "      \n",
    "      \n",
    "      #Among the shown levels of an attribute, select the ones to keep or exclude:\n",
    "      Read_cdtn <- readline(\"Do you want to keep or exclude few conditions? K/E: \")\n",
    "      if( Read_cdtn==\"K\"){\n",
    "        temp <- as.double(unlist(strsplit(readline(\"Enter the index numbers of condition(s) you want to KEEP (separated by commas):\"), split=',')))\n",
    "        ty <- class(metatable[,Condition[i]])\n",
    "        list_keep <-Levels_Cdtn[temp, drop=F]\n",
    "        list_exc <-Levels_Cdtn[-temp, drop=F]\n",
    "        cat(\"The condition(s) you want to exclude in \",colnames(metatable)[Condition[i]],\" :\",list_exc,\"\\n\")\n",
    "        cat(\"The condition(s) you want to keep in \",colnames(metatable)[Condition[i]],\" :\",list_keep,\"\\n\")\n",
    "        \n",
    "        }else if(Read_cdtn==\"E\"){\n",
    "          temp <- as.double(unlist(strsplit(readline(\"Enter the index numbers of condition(s) you want to EXCLUDE (separated by commas):\"), split=',')))\n",
    "          ty <- class(metatable[,Condition[i]])\n",
    "          list_exc <-Levels_Cdtn[temp, drop=F]\n",
    "          list_keep <-Levels_Cdtn[-temp, drop=F]\n",
    "          cat(\"The condition(s) you want to exclude in \",colnames(metatable)[Condition[i]],\" :\",list_exc,\"\\n\")\n",
    "          cat(\"The condition(s) you want to keep in \",colnames(metatable)[Condition[i]],\" :\",list_keep,\"\\n\")\n",
    "        \n",
    "          }else{\n",
    "            print(\"Sorry! You have given a wrong input!! Please enter either K or E\")\n",
    "            break\n",
    "      }\n",
    "      \n",
    "      #In order to keep the original datatype of the columns, we define the following conditions, else it would all become 'characters or factors'\n",
    "      if(ty==\"integer\"){list_keep <- as.integer(list_keep);list_exc <- as.integer(list_exc)\n",
    "      }else if(ty==\"double\"){list_keep <- as.double(list_keep);list_exc <- as.double(list_exc)\n",
    "      }else if(ty==\"numeric\"){list_keep <- as.numeric(list_keep);list_exc <- as.numeric(list_exc)}\n",
    "      \n",
    "      #Gets all the elements in list_keep into list_final \n",
    "      for(j in 1:length(list_keep)){\n",
    "        sub_list <- metatable[(metatable[,Condition[i]] == list_keep[j]),]\n",
    "        list_final <- rbind(list_final,sub_list)\n",
    "      }\n",
    "      metatable <- list_final #list_final again called as metatable in order to keep it in the for-loop for further subsetting\n",
    "    }\n",
    "    return(metatable)\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4eb673",
   "metadata": {},
   "source": [
    "3. <b>Function FrequencyPlot:</b></br>\n",
    "<p style='text-align: justify;'><font color =\"blue\">The below function takes two input datatables: for example, gapfilled and non-gapfilled as its inputs</font>, calculates the frequency distribution of the data in the order of 10 and produces a grouped barplot showing the distribution as output. The frequency plot shows where the features are present in higher number.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689e213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function: FrequencyPlot\n",
    "FrequencyPlot <- function(x1,x2){\n",
    "  \n",
    "   #creating bins from -1 to 10^10 using sequence function seq()\n",
    "    bins <- c(-1,0,(1 * 10^(seq(0,10,1)))) \n",
    "    \n",
    "    #cut function cuts the give table into its appropriate bins\n",
    "    scores_x1 <- cut(as.matrix(x1),bins,labels = c('0','1','10','1E2','1E3','1E4','1E5','1E6','1E7','1E8','1E9','1E10')) \n",
    "    \n",
    "    #transform function convert the tables into a column format: easy for visualization \n",
    "    Table_x1<-transform(table(scores_x1)) #contains 2 columns: \"scores_x1\", \"Freq\"\n",
    "    \n",
    "    #Repeating the same steps for x2\n",
    "    scores_x2 <- cut(as.matrix(x2),bins,labels = c('0','1','10','1E2','1E3','1E4','1E5','1E6','1E7','1E8','1E9','1E10'))\n",
    "    Table_x2 <- transform(table(scores_x2))\n",
    "  \n",
    "    #Getting the names of x1 and x2\n",
    "    arg1 <- deparse(substitute(x1))\n",
    "    arg2 <- deparse(substitute(x2))\n",
    "    \n",
    "    #Creating a data frame for plotting\n",
    "    data_plot <- as.data.frame(c(Table_x1$Freq,Table_x2$Freq)) #Concatenating the frequency info of both tables rowwise\n",
    "    colnames(data_plot) <- \"Freq\" #naming the 1st column as 'Freq'\n",
    "    data_plot$Condition <- c(rep(arg1,12),rep(arg2,12)) #adding a 2nd column 'Condition', which just repeats the name of x1 and x2 accordingly\n",
    "    data_plot$Range_bins <- rep(Table_x1$scores_x1,2) #Adding 3rd column 'Range Bins'\n",
    "    data_plot$Log_Freq <- log(data_plot$Freq+1) #Log scaling the frequency values\n",
    "    \n",
    "    ## GGPLOT2\n",
    "    BarPlot <- ggplot(data_plot, aes(Range_bins, Log_Freq, fill = Condition)) + \n",
    "    geom_bar(stat=\"identity\", position = \"dodge\", width=0.4) + \n",
    "    scale_fill_brewer(palette = \"Set1\") +\n",
    "    ggtitle(label=\"Frequency plot\") +\n",
    "    xlab(\"Range\") + ylab(\"(Log)Frequency\") + labs(fill = \"Data Type\") + \n",
    "    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +   # setting the angle for the x label\n",
    "    theme(axis.text.y = element_text(angle = 45, vjust = 0.5, hjust=1)) +   # setting the angle for the y label\n",
    "    theme(plot.title = element_text(hjust = 0.5)) # centering the plot title\n",
    "  \n",
    "    print(BarPlot)\n",
    "}  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f45b589",
   "metadata": {
    "id": "4f45b589"
   },
   "source": [
    "# Splitting the data into Blanks and Samples using Metadata:\n",
    "<a id=\"data_split\"></a>\n",
    "\n",
    "For the first step: Blank removal, we need to split the data as spectra obtained from blanks and samples respectively using the metadata. More about Blank removal in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6210ac",
   "metadata": {
    "id": "de6210ac"
   },
   "source": [
    "<p style='text-align:justify;'> Before doing anything, let's have a quick look at the metadata by running the next cell. It gives us the information of the multiple levels within each attribute in the metadata. Also check the dimensions to see the number of rows and columns in the metadata. The number of files for B.sub = 40, E.coli = 40, Media =2, hence in total, 82 rows and 6 columns of attributes. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b2c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "InsideLevels(new_md)\n",
    "dim(new_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49149fa9",
   "metadata": {},
   "source": [
    "### Subsetting the dataset:\n",
    "<p style='text-align:justify;'> When having several groups in the data, as in our case, different species like B.sub and E.coli, we can subset the data accordingly using the next cell. For example: We can split it as B.sub + Media and/or E.coli + Media. We need to keep the media here as these are the blanks and their features need to be removed from that of the samples. This would be done in the Blank removal step.</p>\n",
    "\n",
    "<p style='text-align:justify;'> <b>For subsetting:</b> The users can choose the first attribute and <b>keep B.sub and Media</b> (or) simply <b>exclude E.coli.</b> You can also repeat the steps later with E.coli & Media. You can also not subset anything here by skipping the next 2 cells and move on with the code for splitting the data into Samples and blanks. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fbccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_data <- SubsetLevels(new_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3a0379",
   "metadata": {},
   "source": [
    "Lets see if it has been subsetted properly! Also check the dimensions to see if the number of rows in the metadata has been reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abc1a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "InsideLevels(subset_data)\n",
    "dim(subset_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fe2ef5",
   "metadata": {
    "id": "68fe2ef5"
   },
   "source": [
    "Once we subset the data according to a group, we can further proceed to split the blanks from the sample in the cell below. As mentioned before, if no subsetting is involved, you can simply split your metadata into blank and sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c8af3d",
   "metadata": {
    "id": "e0c8af3d"
   },
   "outputs": [],
   "source": [
    "#If subset_data exists, it will take it as \"data\", else take Meta_filter as \"data\"\n",
    "if(exists(\"subset_data\")==T){data <-subset_data}else{data <-new_md}\n",
    "\n",
    "Condition <- as.double(unlist(readline(\"Enter the index number of the attribute to split sample and blank:\")))\n",
    "Levels_Cdtn <- levels(droplevels(as.factor(data[,Condition[1]])))\n",
    "IRdisplay::display(data.frame(INDEX=c(1:length(Levels_Cdtn)),LEVELS=Levels_Cdtn))\n",
    " \n",
    "#Among the shown levels of an attribute, select the ones to keep\n",
    "Blk_id <- as.double(unlist(readline(\"Enter the index number of your BLANK:\")))\n",
    "paste0('You chosen blank is:',Levels_Cdtn[Blk_id])\n",
    "  \n",
    "#Splitting the data into blanks and samples based on the metadata\n",
    "md_Blank <- data[(data[,Condition] == Levels_Cdtn[Blk_id]),]\n",
    "Blank <- new_ft[,which(colnames(new_ft)%in%rownames(md_Blank)),drop=F] \n",
    "md_Samples <- data[(data[,Condition] != Levels_Cdtn[Blk_id]),]\n",
    "Samples <- new_ft[,which(colnames(new_ft)%in%rownames(md_Samples)),drop=F] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed01148",
   "metadata": {
    "id": "aed01148"
   },
   "outputs": [],
   "source": [
    "head(Blank)\n",
    "dim(Blank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37531ef9",
   "metadata": {
    "id": "37531ef9"
   },
   "outputs": [],
   "source": [
    "head(Samples)\n",
    "dim(Samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105ce796",
   "metadata": {
    "id": "105ce796"
   },
   "source": [
    "**Now that we have our data ready, we can start with the cleanup steps!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43099574",
   "metadata": {
    "id": "43099574"
   },
   "source": [
    "# Step1: Blank Removal\n",
    "\n",
    "<p style='text-align: justify;'> In LC-MS/MS, we use solvents called Blanks which are usually injected time-to-time to prevent carryover of the sample. The features coming from these Blanks would also be detected by LC-MS/MS instrument. Our goal here is to remove these features from our samples. The other blanks that can be removed are: Signals coming from growth media alone in terms of microbial growth experiment, signals from the solvent used for extraction methods and so on. Therefore, it is best practice to measure mass spectra of these blanks as well in addition to your sample spectra. </p>\n",
    "\n",
    "**How do we remove these blank features?** </br> \n",
    "<p style='text-align: justify;'> Since we have the feature table split into Control blanks and Sample groups now, we can compare blanks to the sample to identify the background features coming from blanks. A common filtering method is to use a cutoff to remove features that are not present sufficient enough in our biological samples. </p>\n",
    "\n",
    "The steps followed in the next few cells are:\n",
    "1. <p style='text-align: justify;'> We find an average for all the feature intensities in your blank set and sample set. Therefore, for n no.of features in a blank or sample set, we get n no.of averaged features. </p>\n",
    "2. <p style='text-align: justify;'> Next, we get a ratio of this average_blanks vs average_sample. This ratio Blank/sample tells us how much of that particular feature of a sample gets its contribution from blanks. If it is more than 30% (or Cutoff as 0.3), we consider the feature as noise. </p>\n",
    "3. <p style='text-align: justify;'> The resultant information (if ratio > Cutoff or not) is stored in a bin such as 1 = Noise or background signal, 0 = Feature Signal</p>\n",
    "4. <p style='text-align: justify;'> We count the no.of features in the bin that satisfies the condition ratio > cutoff, and consider those features as 'noise or background features' and remove them. </p>\n",
    "\n",
    "For a dataset containing several groups, the filtering steps are performed group-wise using the previous section [Subsetting the dataset](#data_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eaf47e",
   "metadata": {
    "id": "74eaf47e"
   },
   "source": [
    "**<font color='red'> The Cutoff used to obtain the all the files in MZmine Results folder is 0.3 </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25ea36a",
   "metadata": {
    "id": "b25ea36a"
   },
   "outputs": [],
   "source": [
    "if(readline('Do you want to perform Blank Removal- Y/N:')=='Y'){\n",
    "    \n",
    "    #When cutoff is low, more noise (or background) detected; With higher cutoff, less background detected, thus more features observed\n",
    "    Cutoff <- as.numeric(readline('Enter Cutoff value between 0.1 & 1:')) # (i.e. 10% - 100%). Ideal cutoff range: 0.1-0.3\n",
    "    \n",
    "    #Getting mean for every feature in blank and Samples\n",
    "    Avg_blank <- rowMeans(Blank, na.rm= FALSE, dims = 1) # set na.rm = FALSE to check if there are NA values. When set as TRUE, NA values are changed to 0\n",
    "    Avg_samples <- rowMeans(Samples, na.rm= FALSE, dims = 1)\n",
    "    \n",
    "    #Getting the ratio of blank vs Sample\n",
    "    Ratio_blank_Sample <- (Avg_blank+1)/(Avg_samples+1)\n",
    "    \n",
    "    # Creating a bin with 1s when the ratio>Cutoff, else put 0s\n",
    "    Bg_bin <- ifelse(Ratio_blank_Sample > Cutoff, 1, 0 )\n",
    "    Blank_removal <- cbind(Samples,Bg_bin)\n",
    "\n",
    "    # Checking if there are any NA values present. Having NA values in the 4 variables will affect the final dataset to be created\n",
    "    temp_NA_Count <-cbind(Avg_blank ,Avg_samples,Ratio_blank_Sample,Bg_bin)\n",
    "    \n",
    "    print('No of NA values in the following columns:')\n",
    "    print(colSums(is.na(temp_NA_Count)))\n",
    "\n",
    "     #Calculating the number of background features and features present\n",
    "    print(paste(\"No.of Background or noise features:\",sum(Bg_bin ==1,na.rm = TRUE)))\n",
    "    print(paste(\"No.of features after excluding noise:\",(nrow(Samples) - sum(Bg_bin ==1,na.rm = TRUE)))) \n",
    "\n",
    "    Blank_removal <- Blank_removal %>% filter(Bg_bin == 0) # Taking only the feature signals\n",
    "    Blank_removal <- as.matrix(Blank_removal[,-ncol(Blank_removal)]) # removing the last column Bg_bin \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080215e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "write.csv(Blank_removal, file.path(fName,paste0('Blank_removed.csv')),row.names =TRUE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cbd3f6",
   "metadata": {
    "id": "81cbd3f6"
   },
   "outputs": [],
   "source": [
    "head(Blank_removal)\n",
    "dim(Blank_removal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4210007",
   "metadata": {
    "id": "b4210007"
   },
   "source": [
    "# Step 2: Imputation\n",
    "\n",
    "<p style='text-align: justify;'> For several reasons, real world datasets might have some missing values in it, in the form of NA, NANs or 0s. Eventhough the gapfilling step of MZmine fills the missing values, we still end up with some missing values or 0s in our feature table. This could be problematic for statistical analysis. </p> \n",
    "<p style='text-align: justify;'> In order to have a better dataset, we cannot simply discard those rows or columns with missing values as we will lose a chunk of our valuable data. Instead we can try imputing those missing values. Imputation involves replacing the missing values in the data with a meaningful, reasonable guess. There are several methods, such as: </p> \n",
    "  \n",
    "1) Mean imputation (replacing the missing values in a column with the mean or average of the column)  \n",
    "2) Replacing it with the most frequent value  \n",
    "3) Several other machine learning imputation methods such as k-nearest neighbors algorithm(k-NN), Hidden Markov Model(HMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2ee8a6",
   "metadata": {
    "id": "cb2ee8a6"
   },
   "source": [
    "## 1) Imputation using Non-gapfilled feature table:\n",
    "One such method, we are going to use is: **to replace the zeros from the gapfilled quant table with the non-gap filled table** we get from MZmine. In order to do that, we can visualize our data distribution using the frequenct plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d83ae53",
   "metadata": {
    "id": "9d83ae53"
   },
   "outputs": [],
   "source": [
    "GapFilled <- Blank_removal\n",
    "if(exists(\"nft\")==T){NotGapFilled <- nft}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e527abdb",
   "metadata": {
    "id": "e527abdb"
   },
   "outputs": [],
   "source": [
    "if(readline('Do you want to perform Imputation with minimum value of NonGapFilled table? - Y/N:')=='Y'){\n",
    "    plot<- FrequencyPlot(GapFilled,NotGapFilled)\n",
    "    \n",
    "    Arg1 = plot$data$Condition[1]\n",
    "    Arg2 = plot$data$Condition[13]\n",
    "    \n",
    "    # getting the 2nd minimum value of non-gap filled data. (The first minimum value in the data table is usually zero)\n",
    "    RawLOD <- round(min(NotGapFilled[NotGapFilled!=min(NotGapFilled)]))\n",
    "    \n",
    "    print(paste0(\"The minimum value greater than 0 for \",Arg1,\":\", round(min(GapFilled[GapFilled!=min(GapFilled)]))))\n",
    "    print(paste0(\"The minimum value greater than 0 for \",Arg2,\":\", RawLOD))\n",
    "\n",
    "    Imputed <- GapFilled\n",
    "    Imputed[Imputed<RawLOD] <- RawLOD # Replacing values<RawLOD with RawLOD\n",
    "    head(Imputed) \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4916143",
   "metadata": {
    "id": "f4916143"
   },
   "outputs": [],
   "source": [
    "write.csv(Imputed, file.path(fName,paste0('Imputed_QuantTable_filled_with_MinValue_',RawLOD,'_CutOff_Used_',Cutoff,'.csv')),row.names =TRUE) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06030071",
   "metadata": {
    "id": "06030071"
   },
   "source": [
    "## 2) Imputation with a Cutoff LOD:\n",
    "<a id=\"Impute_LOD\"></a>\n",
    "\n",
    "<p style='text-align: justify;'>Instead of replacing with the minimum value of nft, we can also only use ft and see the frquency distribution of its features with the frequency plot. The frequency plot shows where the features are present in higher number.</p>\n",
    "\n",
    "<p style='text-align: justify;'>For ex: If until range 10-100, (shown in the figure as 1E2) there are no or very less features, we want to exclude until that range and consider from range (100-1000), or, in other words, '1E3' or '1000' as Cutoff_LOD. This value will be used to replace the zeros as well as all the values lower than Cutoff_LOD in the data table.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bd2928",
   "metadata": {
    "id": "e0bd2928"
   },
   "outputs": [],
   "source": [
    "FrequencyPlot(GapFilled,GapFilled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3efacf3",
   "metadata": {},
   "source": [
    "However, if you are not satisfied with the value as shown in the plot, you can enter any value for imputation. For ex: The minimum value is GapFilled table, which in our case is 1175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f8f811",
   "metadata": {
    "id": "30f8f811"
   },
   "outputs": [],
   "source": [
    "if(readline('Do you want to perform Imputation with a Cutoff LOD? - Y/N:')=='Y'){\n",
    "    Cutoff_LOD <- as.numeric(readline(\"Enter your Cutoff LOD as seen in the plot or your desired value for imputation:\"))  #Enter the LOD value as seen in the frequency plot\n",
    "    Imputed <- GapFilled\n",
    "    Imputed[Imputed <Cutoff_LOD] <- Cutoff_LOD\n",
    "    head(Imputed)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67865b8",
   "metadata": {
    "id": "a67865b8"
   },
   "outputs": [],
   "source": [
    "dim(Imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed071fa7",
   "metadata": {
    "id": "ed071fa7"
   },
   "outputs": [],
   "source": [
    "write.csv(Imputed,file.path(fName, paste0('Imputed_QuantTable_filled_with_',Cutoff_LOD,'_CutOff_Used_',Cutoff,'_Bsub','.csv')),row.names =TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f428856",
   "metadata": {
    "id": "8f428856"
   },
   "outputs": [],
   "source": [
    "#removing all the rows with only cutoff values:\n",
    "#Imputed<-Imputed[rowMeans(Imputed)!= Cutoff_LOD,]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06a2dd7",
   "metadata": {
    "id": "f06a2dd7"
   },
   "source": [
    "# Step 3:Normalization\n",
    "The following code performs sample-centric (column-wise) normalisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a008a7a",
   "metadata": {
    "id": "3a008a7a"
   },
   "outputs": [],
   "source": [
    "if (readline(\"Do you want to perform Normalization: Y/N:\") == 'Y'){\n",
    "    #Getting column-wise sums of the input-data\n",
    "    sample_sum <- colSums(Imputed, na.rm= TRUE, dims = 1)\n",
    "    \n",
    "    #Dividing each element of a particular column with its column sum\n",
    "    Normalized_data <- c()\n",
    "    for (i in 1:ncol(Imputed)){\n",
    "        x <- Imputed[,i] / sample_sum[i]\n",
    "        Normalized_data <- cbind(Normalized_data, x)\n",
    "    }\n",
    "    colnames(Normalized_data) <- names(sample_sum)\n",
    "    \n",
    "    head(Normalized_data)\n",
    "} else return(head(Imputed))\n",
    "  \n",
    "print(paste('No.of NA values in Normalized data:',sum(is.na(Normalized_data)== TRUE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2539f80c",
   "metadata": {
    "id": "2539f80c"
   },
   "outputs": [],
   "source": [
    "dim(Normalized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fc407e",
   "metadata": {
    "id": "58fc407e"
   },
   "outputs": [],
   "source": [
    "write.csv(Normalized_data, file.path(fName,'Normalised_Quant_table_Bsub_OnlyCutOffsRemoved.csv'),row.names =TRUE) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4628109",
   "metadata": {},
   "source": [
    "All the result files would be available in the 'Result' folder within your working directory (i.e the folder path you mentioned in the beginning of the script)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f57cd7",
   "metadata": {},
   "source": [
    "### Getting output files from Google Colab:\n",
    "<a id=\"colab_output\"></a>\n",
    "For Google Collab users, we can zip the result folder which contains all the output files using the next cell and download the zip file directly from the folder \"/content/My_TestData\" into the local system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f241251",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only for Google Colab\n",
    "zip(zipfile = 'TestData_Results', files = \"/content/My_TestData_Results/\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of Data_CleanUp_Metabolomics.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/abzer005/Data_Cleanup/blob/main/Data_CleanUp_Metabolomics.ipynb",
     "timestamp": 1659601954113
    }
   ]
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
